# Bellman equation
The Bellman equation is a formula that helps the agent to make good decision. There are two forms, the Bellman Expectation equation and Bellman Optimality equation. The expectation equation takes current state and action as input and produce expected value as output. The optimality equation takes current state and action as input and produce optimal value as output. The Bellman expectation equation is `Value(state) = Expected value + Discount value of next state = (Reward of next state) + (gamma * Value(state+1))`
# Other approaches
There are other approaches to learn the policy function like *Monte Carlo* and *Temporal Difference Learning*. Monte Carlo method uses entire episode experience to learn the policy. Temporal Difference Learning uses only one sequence (State,Action,Reward,State1) to learn the policy.  

# About Q-Learning
* It is a off policy value based method
	* off policy - different policy for acting (inference) and updating (training)
	* In Q-learning we use epsilon greedy policy(both exploration and exploitation) in acting and greedy policy in training (only exploitation)
	* on policy - same policy for acting and updating
	* In SARSA , epsilon greedy policy is used in both
* It uses Temporal Difference learning approach to train its action value function
* In Q-learning we used to train the *Q function*, Q means **Quality**
* The Q-function takes current state and action as input and produce value as output
* Q function contains a *Q-table* that stores state-action value , it contains actions as columns and state as rows
# Algorithm
1. Initialize the Q-table with 0.
2. Choose an action based on epsilon-greedy strategy
	1. This strategy handles the exploration exploitation tradeoff
	2. If the epsilon value is 1 by default then
		1. With probability 1 - epsilon that is 0 then it will do exploitation
		2. With probability epsilon , it will do exploration
	3. During training the epsilon value is larger so it will explore and it gradually reduces.
3. Perform the action A , get reward R and move to next state S
4. Update the table with Q(current state, current action)